{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/14 10:14:31 WARN Utils: Your hostname, MacBook-Air-Mufida.local resolves to a loopback address: 127.0.0.1; using 192.168.1.31 instead (on interface en0)\n",
      "22/03/14 10:14:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/14 10:14:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import pyspark.sql.utils\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"NYC Taxi Trip\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "vendor_dict = {'1': 'llc', '2': 'verifone'}\n",
    "rate_code_dict = {\n",
    "    '1': 'standard rate',\n",
    "    '2': 'jfk',\n",
    "    '3': 'newark',\n",
    "    '4': 'nassau',\n",
    "    '5': 'negotiated fare',\n",
    "    '6': 'group ride'\n",
    "}\n",
    "payment_type_dict = {\n",
    "    '1': 'credit card',\n",
    "    '2': 'cash',\n",
    "    '3': 'no charge',\n",
    "    '4': 'dispute',\n",
    "    '5': 'unknown',\n",
    "    '6': 'voided trip'\n",
    "}\n",
    "trip_type_dict = {'1': 'street hall', '2': 'dispatch'}\n",
    "store_forward_dict = {'Y': 'True', 'N': 'False'}\n",
    "\n",
    "taxi_zone_df = pd.read_csv('taxi_zone.csv', usecols=['LocationID','Zone'])\n",
    "zone_dict = taxi_zone_df.set_index('LocationID').T.to_dict('records')[0]\n",
    "zone_dict[265] = 'Unknown'\n",
    "zone_dict = {str(k):str(v) for k,v in zone_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mapping(taxi_type, year, fmonth):\n",
    "    src_path = f'tmp/raw/{taxi_type}/{year}/{fmonth}/*'\n",
    "    df = spark.read \\\n",
    "        .options(header='True', delimeter=',') \\\n",
    "        .csv(src_path)\n",
    "\n",
    "    df = df.replace(store_forward_dict,subset=['store_and_fwd_flag']) \\\n",
    "            .withColumn('store', col('store_and_fwd_flag').cast(BooleanType())) \\\n",
    "            .withColumn('forward', col('store_and_fwd_flag').cast(BooleanType())) \\\n",
    "            .drop('store_and_fwd_flag') \\\n",
    "            .replace(vendor_dict,subset=['VendorID']) \\\n",
    "            .replace(rate_code_dict,subset=['RatecodeID']) \\\n",
    "            .replace(payment_type_dict,subset=['payment_type']) \\\n",
    "            .replace(zone_dict,subset=['PULocationID']) \\\n",
    "            .replace(zone_dict,subset=['DOLocationID']) \\\n",
    "            .withColumn('taxi_type', lit(taxi_type))\n",
    "    return df\n",
    "\n",
    "def change_schema(col_list, df):\n",
    "    df = df.toDF(*col_list)\n",
    "    df = df.withColumn(\"pickup_datetime\",to_timestamp(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"dropoff_datetime\",to_timestamp(\"dropoff_datetime\")) \\\n",
    "            .withColumn(\"passenger_count\",col(\"passenger_count\").cast(IntegerType())) \\\n",
    "            .withColumn(\"trip_distance\",col(\"trip_distance\").cast(DoubleType())) \\\n",
    "            .withColumn(\"fare_amount\",col(\"fare_amount\").cast(DoubleType())) \\\n",
    "            .withColumn(\"extra\",col(\"extra\").cast(DoubleType())) \\\n",
    "            .withColumn(\"mta_tax\",col(\"mta_tax\").cast(DoubleType())) \\\n",
    "            .withColumn(\"tip_amount\",col(\"tip_amount\").cast(DoubleType())) \\\n",
    "            .withColumn(\"tolls_amount\",col(\"tolls_amount\").cast(DoubleType())) \\\n",
    "            .withColumn(\"improvement_surcharge\",col(\"improvement_surcharge\").cast(DoubleType())) \\\n",
    "            .withColumn(\"total_amount\",col(\"total_amount\").cast(DoubleType())) \\\n",
    "            .withColumn(\"congestion_surcharge\",col(\"congestion_surcharge\").cast(DoubleType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(year):\n",
    "    for month in range(1,13):\n",
    "        try:\n",
    "            fmonth = \"{:02d}\".format(month)\n",
    "            dest_path = f'tmp/pq_clean/{year}/{fmonth}/'\n",
    "\n",
    "            #transform dataframe\n",
    "            df_yellow = transform_mapping(\"yellow\", year, fmonth)\n",
    "            df_green = transform_mapping(\"green\", year, fmonth)\n",
    "            df_green = df_green.drop(\"ehail_fee\",\"trip_type\")\n",
    "\n",
    "            #change schema of dataframe\n",
    "            yellow_col_list = [\"vendor\",\"pickup_datetime\",\"dropoff_datetime\",\"passenger_count\",\"trip_distance\",\"ratecode\",\"pickup_location\",\"dropoff_location\",\"payment_type\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"congestion_surcharge\",\"store\",\"forward\",\"taxi_type\"]\n",
    "            green_col_list = [\"vendor\",\"pickup_datetime\",\"dropoff_datetime\",\"ratecode\",\"pickup_location\",\"dropoff_location\",\"passenger_count\",\"trip_distance\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"payment_type\",\"congestion_surcharge\",\"store\",\"forward\",\"taxi_type\"]\n",
    "            df_yellow = change_schema(yellow_col_list, df_yellow)\n",
    "            df_green = change_schema(green_col_list, df_green)\n",
    "\n",
    "            #combine yellow taxi df and green taxi df\n",
    "            df = df_yellow.unionByName(df_green)\n",
    "\n",
    "            #load to tmp as parquet\n",
    "            df.repartition(4).write.parquet(dest_path, mode='overwrite')\n",
    "            print(f\"tripdata_{year}_{month}.csv transformed\")\n",
    "\n",
    "        except pyspark.sql.utils.AnalysisException:\n",
    "            print(f\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n",
      "File not found\n",
      "File not found\n",
      "File not found\n",
      "File not found\n"
     ]
    }
   ],
   "source": [
    "transform(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
